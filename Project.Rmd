---
title: "Projet Stats"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align="center", results="hold")
library(ggplot2)
library(esquisse)
```

# Data analysis

## Ababakar : Description

```{r}
# Lecture jeu de données
setwd("/home/leo/GoogleDrive/Cours/INSA_4A/Projet")
data = read.table("DataEnergy-Student.csv", header = TRUE, sep = ",")
data
# Mise sous forme de facteur des données catégorielles
data$Energy.efficiency = as.factor(data$Energy.efficiency)
data$Glazing.area.distr = as.factor(data$Glazing.area.distr)
data$orientation = as.factor(data$orientation)
# Correction du jeu de données
data$Glazing.area[which(data$Glazing.area.distr == 0)] = 0
# Infos sur le jeu de données
str(data)
```
Le jeu de données est composé de 768 batiments avec chacun 10 variables.

Quantitatives :
- Relative.compactness : compacité relative à un cube de même volume (FORMULE)
- Surface.area : surface totale (sol + murs + toit)
- Wall.area : surface de mur extérieurs
- Roof.area : surface de toit
- Overall.height : hauteur du batiment (soit 3.5 soit 7 m)
- Glazing.area : Surface vitrée (en pourcentage de la surface au sol)
- Load : Energie consommée (sortie du problème)
Catégorielles :
- orientation : orientation de la maison (Nord, Sud, Est, Ouest)
- Glazing.area.distr : Distribution des vitres (Uniforme, ou plus d'un coté)
- Energy.efficiency : Indicateur de l'énergie consommées (de A à G)

```{r}
quanti <- c(1:5, 7, 9)
quali <- c(6, 8, 10) 
pairs(data[, quanti], pch = '.', cex = 0.1, cex.labels = 0.55) 
plot(2*data$Roof.area + data$Wall.area, data$Surface.area)
```


```{r}
summary(data)
```

Même nombre de batiments faisant face à chaque orientation. Même nombre de batiments avec les différentes répartitions de fenêtres, sauf les batiments sans fenêtres.

```{r}
ggplot(data) +
 aes(x = Glazing.area, y = Energy, colour = as.factor(Overall.height)) +
 geom_point(size = 1L) +
 theme_minimal() + geom_smooth(method = "lm")
```

On voit que la surface vitrée à une influence sur l'énergie consommée.

```{r}
boxplot(Energy ~ Overall.height, data = data)
```

## Léo : ACP

```{r}
library("FactoMineR")
data$class2 = as.factor(class2)
data$class4 = as.factor(class4)
data$class12 = as.factor(class12)
res.acp <- PCA(data,scale.unit=T,quali.sup=c(6,8,10,11,12,13),quanti.sup=9,ncp=8, graph=F)
barplot(res.acp$eig[,"percentage of variance"], main="Pourcentage d'inertie")
barplot(res.acp$eig[,"cumulative percentage of variance"], main="Pourcentage d'inertie cumulé")
sum(res.acp$eig[,"percentage of variance"][1:3])
plot(res.acp, choix="var", axes = c(1,2))
plot(res.acp, choix="var", axes = c(2,3))
plot(res.acp, choix="var", axes = c(1,3))
```

```{r}
plot(res.acp, choix="ind", axes = c(1,2), invisible="quali", habillage="Energy.efficiency")
plot(res.acp, choix="ind", axes = c(2,3), invisible="quali", habillage="Energy.efficiency")
plot(res.acp, choix="ind", axes = c(1,3), invisible="quali", habillage="Energy.efficiency")
```

Première composante principale : Hauteur, compacité (en positif) et surface totale, surface de toit (en négatif)
Deuxième composante principale : Surface de murs
Troisème composante principale : Surface vitrée

## Quentin : clustering
### Analyse visuelle

```{r}
ggplot(data) +
 aes(x = Relative.compactness, y = Energy) +
 geom_point(size = 1L, colour = "#0c4c8a") +
 theme_minimal()
ggplot(data) +
 aes(x = Glazing.area, y = Energy) +
 geom_point(size = 1L, colour = "#0c4c8a") +
 theme_minimal()
```

Visuellement, c'est selon la compacité relative et la surface vitrée que le distingue le mieux des groupes.
Selon la compacité relative, on constate principalement deux groupes. On peut également voir 12 groupes mais ceux-ci sont moins évidents.
Selon la surface vitrée, on distingue principalement 4 groupes.

### Clustering hiérarchique
Après cette analyse visuelle, nous allons utiliser des méthodes de clustering pour confirmer ou infirmer nos observations. Commençons par le clustering hiérarchique.

```{r}
hc = hclust(dist(data[c(1,2,3,4,5,7)]), method = "ward.D2")
plot(hc)
abline(h=3400,col="red")
abline(h=1300,col="red")
abline(h=880,col="red")
abline(h=200,col="red")
plot(sort(hc$height, decreasing = TRUE)[1:20])
abline(v=1.5,col="red")
abline(v=3.5,col="red")
abline(v=4.5,col="red")
abline(v=11.5,col="red")
```
Sur le dendogramme, deux classes apparaissent clairement. On peut aussi distinguer 4,5 ou 12 classes.
Sur le graphique de la variance inter-classe, on observe un saut important au passage à 2 classes. On retrouve également les résultats que l'on a obtenu avec le dendogramme en observant des sauts aux passages à 4, 5 et 12 classes.

Ces premiers outils semblent confirmer les nombres de classes qui semblent pertinents : 2, 4 et 12.

On réalise donc dans un premier temps un découpage en 2, 4 et 12 classes selon le clustering hiérarchique. On compare ces résultats avec nos observations selon plusieurs variables explicatives.

```{r}
class2 = cutree(hc, k = 2)
ggplot(data) +
 aes(x = Relative.compactness, y = Energy) +
 geom_point(size = 1L, colour = class2) +
 theme_minimal()
class4 = cutree(hc, k = 4)
ggplot(data) +
 aes(x = Relative.compactness, y = Energy) +
 geom_point(size = 1L, colour = class4) +
 theme_minimal()
class12 = cutree(hc, k = 12)
ggplot(data) +
 aes(x = Relative.compactness, y = Energy) +
 geom_point(size = 1L, colour = class12) +
 theme_minimal()
```

Visuellement selon la compacité relative, le découpage en 2 groupes et celui en 12 groupes semblent très pertinents. Celui en 4 groupes ne semble pas s'expliquer par la compacité relative.

```{r}
class2 = cutree(hc, k = 2)
ggplot(data) +
 aes(x = Glazing.area, y = Energy) +
 geom_point(size = 1L, colour = class2) +
 theme_minimal()
class4 = cutree(hc, k = 4)
ggplot(data) +
 aes(x = Glazing.area, y = Energy) +
 geom_point(size = 1L, colour = class4) +
 theme_minimal()
class12 = cutree(hc, k = 12)
ggplot(data) +
 aes(x = Glazing.area, y = Energy) +
 geom_point(size = 1L, colour = class12) +
 theme_minimal()
```

Selon la surface vitrée, seul le découpage en 2 classes semble pertinent. On aurait pu s'attendre à un découpage en 4 classes pertinents sur ce graphique mais ce n'est pas le cas.

```{r}
class2 = cutree(hc, k = 2)
ggplot(data) +
 aes(x = Wall.area, y = Energy) +
 geom_point(size = 1L, colour = class2) +
 theme_minimal()
class4 = cutree(hc, k = 4)
ggplot(data) +
 aes(x = Wall.area, y = Energy) +
 geom_point(size = 1L, colour = class4) +
 theme_minimal()
class12 = cutree(hc, k = 12)
ggplot(data) +
 aes(x = Wall.area, y = Energy) +
 geom_point(size = 1L, colour = class12) +
 theme_minimal()
```

Finalement, le découpage en 4 classes semble pertinent selon la surface des murs.

### k-means

On utilise à présent un autre outil : k-means. Comparons les résultats avec ceux du clustering hiérarchique.

```{r}
# k-means à 2 classes
kmres2 = kmeans(data[,c(1:5,7)], centers = 2)
kmclus2 = kmres2$cluster

pairs(data[,quanti], col = kmclus2)

ggplot(data) +
 aes(x = Relative.compactness, y = Energy) +
 geom_point(size = 1L, colour = kmclus2) +
 theme_minimal()

# clustering hierarchique à 2 classes
ggplot(data) +
 aes(x = Relative.compactness, y = Energy) +
 geom_point(size = 1L, colour = class2) +
 theme_minimal()
```

Pour le découpage à deux classes, les deux méthodes fournissent exactement les mêmes résultats.

```{r}
# k-means à 4 classes
kmres4 = kmeans(data[,c(1:5,7)], centers = 4)
kmclus4 = kmres4$cluster

pairs(data[,quanti], col = kmclus4)

ggplot(data) +
 aes(x = Wall.area, y = Energy) +
 geom_point(size = 1L, colour = kmclus4) +
 theme_minimal()

# clustering hierarchique à 4 classes
ggplot(data) +
 aes(x = Wall.area, y = Energy) +
 geom_point(size = 1L, colour = class4) +
 theme_minimal()
```

Après plusieurs exécutions, k-means ne presque jamais le même résultat que le clustering hiérarchiques. Alors que le clustering hiérarchiques semble créer des classes pertinentes selon la surface des murs, le k-means réalise souvent un découpage moins pertinent. En effet, le découpage issu de k-means consiste souvent à séparer en deux les données, puis à séparer en trois un des deux groupes obtenus de façon assez arbitraire. Le découpage en 4 classes avec k-means est donc peu pertinent.


```{r}
# k-means à 12 classes
kmres12 = kmeans(data[,c(1:5,7)], centers = 12)
kmclus12 = kmres12$cluster

pairs(data[,quanti], col = kmclus12)

ggplot(data) +
 aes(x = Relative.compactness, y = Energy) +
 geom_point(size = 1L, colour = kmclus12) +
 theme_minimal()

# clustering hierarchique à 12 classes
ggplot(data) +
 aes(x = Relative.compactness, y = Energy) +
 geom_point(size = 1L, colour = class12) +
 theme_minimal()
```

Selon les exécutions, k-means donne ici des résultats assez proches du clustering hiérarchique.

### Visualisation des classes sur les axes de l'ACP

On utilise ici les classes obtenues par clustering hiérarchiques car celles-ci semblaient plus naturelles visuellement.

```{r}
data$class2 = as.factor(class2)
data$class4 = as.factor(class4)
data$class12 = as.factor(class12)
res.acp <- PCA(data,scale.unit=T,quali.sup=c(6,8,10,11,12,13),quanti.sup=9,ncp=8, graph=F)
```


```{r}
plot(res.acp, choix="ind", axes = c(1,2), invisible="quali", habillage="class2")
plot(res.acp, choix="ind", axes = c(2,3), invisible="quali", habillage="class2")
plot(res.acp, choix="ind", axes = c(1,3), invisible="quali", habillage="class2")
```

On voit ici que les données sont bien découpées en 2 classes selon le premier axe. Cela donne une bonne confiance en le découpage en 2 classes.

```{r}
plot(res.acp, choix="ind", axes = c(1,2), invisible="quali", habillage="class4")
plot(res.acp, choix="ind", axes = c(2,3), invisible="quali", habillage="class4")
plot(res.acp, choix="ind", axes = c(1,3), invisible="quali", habillage="class4")
```

Le découpage en 4 classes se fait selon les axes 1 et 2, même si il est moins évident que celui en 2 classes. (notamment sur les classes 3 et 4 ci-dessus).

```{r}
plot(res.acp, choix="ind", axes = c(1,2), invisible="quali", habillage="class12")
plot(res.acp, choix="ind", axes = c(2,3), invisible="quali", habillage="class12")
plot(res.acp, choix="ind", axes = c(1,3), invisible="quali", habillage="class12")
```

Ici, on peut avoir la même observation que pour le découpage en 4 classes : le découpage est moins net que celui en 2 classes et se fait selon les axes 1 et 2.